# Crawler Project

## Preparation

Alright - I think we're good for tomorrow. We're going to try to write a basic crawler and persist some of the data we find.

If you get a chance to review, you can figure out

1. the basics of the http client you'd like to use (how to POST, how to set headers)
1. some JSON lib for parsing
1. how you might persist JSON blobs (either using something like sqlite or just storing on the file system)

You'll also want some browser that has some developer tools extension so that you can see xhr web requests

we're not going to do anything crazy: a couple of functions, a couple of loops, some http requests, and storing JSON somewhere

This role is for a Python engineer, but feel free to use whatever language you're most familiar with (Ruby, C++, Python, etc)

I'll start off sharing my screen, giving you a walkthrough and the basics of the problem; I'll then hand it over to you and you can share your screen - so you'll want some editor and language runtime installed on your laptop

You'll be doing the coding, but it should feel more collaborative than a balls-to-the-wall coding challenge

If you need more time and would like to reschedule for later this week, that's fine too - let me know whatever you need

And if you'd rather do this offline, as a standard coding challenge that you do at your leisure, that's fine too
